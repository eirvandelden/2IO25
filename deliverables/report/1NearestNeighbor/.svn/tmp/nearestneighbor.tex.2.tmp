\chapter{Nearest Neighbor}
\label{cha:nearest_neighbor}

Our initial approach was to solve the reconstruction of the curve with merely a Nearest Neighbor algorithm.\\
We begin with giving some functions on which our algorithm relies, then we give a description of the algorithm itself.

  \section{Lexicographic Smallest}
  \label{sec:nn_lexicographic_smallest}
    
    \subsection{Introduction}
    \label{sub:ls_introduction}
        We introduce \textit{LexicographicSmallest} to find the lexicographic smallest point in a set of points. This will be our point from which the curve reconstruction starts.
        
    \subsection{Definitions}
    \label{sub:ls_definitions}
      \begin{definition} \label{def:ls}
          Given a set $S$ with $n$ points, there exists exactly one lexicographic smallest point in $S$.
      \end{definition}


    \subsection{Functional Description}
    \label{sub:ls_functional_description}
      For the given set $S$ of $n$ points, \textit{LexicographicSmallest} starts by selecting the first point in $S$ and compares its x-coordinate with the x-coordinate of the other points in $S$, until a smaller x-coordinate is found. This point becomes the new lexicographic smallest and the algorithm continues comparing moving right of this point in the array.\\
      When in a comparison the two x-coordinates of the two points are equal, the point with the smaller y-coordinate becomes the lexicographic smallest.\\
      When all $n$ points are processed, the index of the lexicographic smallest point is returned.

    \subsubsection{Proof of Correctness}
    \label{ssub:ls_proof}
      \textit{LexicographicSmallest} compares all the x-coordinates of the points in $S$ and keeps track of the smallest until now. When there are two x-coordinates with the same value, the point with the smaller y-coordinate becomes the lexicographic smallest.\\ So, when all points are processed, the lexicographic smallest point of $S$ is found.


    \subsection{Running Time Analysis}
    \label{sub:ls_running_time_analysis}
      As follows from the functional description, the algorithm processes each point exactly once.\\
      So the running time is $O(n)$.


  \section{Nearest Neighbor}
  \label{sec:nearest_neighbor}

    \subsection{Definitions}
    \label{sub:nn_definitions}
      \begin{definition} \label{def:nn}
          Given a set $S$ of $n$ input points, the nearest neighbor of a point $p$ in $S$ is the point $q$ with the smallest distance to $p$ from all points in $S$.
      \end{definition}

    \subsection{Functional Description}
    \label{sub:nn_functional_description}
      With an array $A$ of $1$ to $n$ points and one empty array $B$ which has the same length, Nearest Neighbor is initially called.\\
      From $A$ the lexicographic smallest point is chosen, from Definition~\ref{def:ls} we know that \textit{LexicographicSmallest} returns only one point, say $p$, and this point $p$ is removed from $A$ and placed into $B$. This point is the point from which to start. Next the lengths between this point and all the remaining points in $A$ are measured and the point with the smallest distance to this point is chosen as to be the nearest neighbor, say $q$.\\
      Now that the nearest neighbor of $p$ is found, $q$ is moved from $A$ to $B$. Then the nearest neighbor for $q$ is found. This process continues until all points except for the last one in $A$ are processed. This point has no nearest neighbor anymore, so is directly moved from $A$ to $B$. When $A$ is empty and $B$ full, $B$ is returned.
      
      \begin{center}
        \begin{tabular}{l l r}
               1  & \textbf{NearestNeighbor}$(P)$                                               & \\
               2  & $ls \gets$ \textbf{LexicographicSmallest}$(P)$                              & \\
               3  & $p \gets P[ls]$                                                             & \\
               4  & \textbf{for} $i \gets 0$ \textbf{to} $|P| - 2$ \textbf{do}                  & \\
               5  & \qquad $P \gets P \backslash \{ p \}$                                       & \\
               6  & \qquad $d \gets \infty$                                                     & \\
               7  & \qquad $q \gets p $                                                         & \\
               8  & \qquad \textbf{for} $j \gets 0$ \textbf{to} $|P| - 1$ \textbf{do}           & \\
               9  & \qquad \qquad \textbf{if} \textbf{distance}$(p, P[j]) < d$ \textbf{then}    & \\
              10  & \qquad \qquad ~~ $d \gets$ \textbf{distance}$(p, P[j])$                     & \\
              11  &  \qquad \qquad ~~ $q \gets P[j]$                                            & \\
              12  &  \qquad \qquad \textbf{fi}                                                  & \\
              13  &  \qquad \textbf{od}                                                         & \\
              14  &  $S \gets S \cup \{ p \}$                                                   & \\
              15  &  $p \gets q$                                                                & \\
              16  &  \textbf{od}                                                                & \\ \hline \hline
          & \qquad \qquad \qquad\qquad\qquad\qquad\qquad \textbf{Total }  & $O(n^{2})$
         \end{tabular}
         \label{fig:nn_pseudo_code}\\
         Figure 2.1: The pseudo code of \textit{NearestNeighbor}
      \end{center}

    \subsubsection{Proof of Correctness}
    \label{ssub:nn_proof_of_correctness}
        \textit{NearestNeighbor} begins with removing $p$ from $S$ and calculates the distances between this point $p$ and the other points in $S$ and keeps track of the smallest point processed so far.\\ So when all points are processed \textit{NearestNeighbor} has found the nearest of point $p$, say $q$.
        Then the algorithm computes the nearest neighbor of $q$, say $r$, in the same way as for $p$ and removes $q$ from $S$.\\ Every time the nearest neighbor of a point is found, the found point is removed from $S$.
        This goes on until the $n-1^{th}$ point is processed; the nearest neighbor of this point is the only point left in set $S$: the $n^{th}$ point.\\

    \subsection{Running Time Analysis}
    \label{sub:nn_running_time_analysis}
        Proving the running time for \textit{NearestNeighbor} is straight forward. There are only three lines that are not constant time; lines 1, 3 and 7 in Figure 2.1.\\
        Line 1 uses a function that searches for the lexicographic smallest for its given input set. As shown before (\ref{sub:ls_running_time_analysis}) this takes $O(n)$ time.\\
        Line 7 starts a for loop that looks through all remaining elements in the set $P$ looking for the smallest distance between point $j$ and given point $p$. This comparison is done in constant time and does not affect the running time of the for loop. In the first run, this for loop needs to go through the most number of elements, namely $n-1$ elements. The running time of this loop is $O(n)$. \\
        Line 3 starts the for loop that runs through all elements in the set, minus the last element. Thus, this takes $O(n-1)$. We need to take into account the nested for loop and finding the lexicographic smallest point, this makes the total running time: \\
        $O(n-1) * O(n) + O(n) =  O(n^{2} - n ) + O(n) = O(n^{2})$ \\
        Thus \textit{NearestNeighbor} runs in $O(n^{2})$.

  \section{Test Results}
  \label{sec:nn_test_results}

    \subsection{Running time}
    \label{sub:nn_running_time}
    We first tested our algorithm on randomly generated points (solely for determining the running time of the algorithm).\\
    We ran the experiments on a HP Compaq 8510w Laptop with the following specifications:\\
    2,4 GHz Intel Core 2 Duo processor T7700\\
    2 GB memory.
    The results are given in the Table 2.1 and Figure 2.2.

      \begin{center}
        \begin{tabular}{|p{2.5cm}|p{2.5cm}|}
            \hline
            Points & Seconds\\
            \hline
            \hline
            500 & 0.015\\
            \hline
            1000 & 0.016\\
            \hline
            1500 & 0.047\\
            \hline
            2000 & 0.078\\
            \hline
            5000 & 0.442\\
            \hline
            10000 & 1.669\\
            \hline
            15000 & 4.087\\
            \hline
            20000 & 6.755\\
            \hline
            30000 & 16.552\\
            \hline
            40000 & 27.878\\
            \hline
            50000 & 47,768\\
            \hline
            60000 & 66.16\\
            \hline
            80000 & 108.358\\
            \hline
            100000 & 202.239\\
            \hline
        \end{tabular}
        \label{tab:nn_runningtime}\\
        Table 2.1: A table of our results on testing \textit{NearestNeighbor}
    \end{center}
        
    \begin{center}
      \includegraphics[scale = 0.7]{1NearestNeighbor/nnRuntimegraph.png}\\
      Figure 2.2: Graph of \textit{NearestNeighbor}'s Running Time
      \label{fig:nn_runningtime}
    \end{center}

    This graph tells us that our analysis of the running time (\ref{sub:nn_running_time_analysis}) is correct: when the number of input points are doubled, the running time is quadrupled $(2^2)$, when they are tripled, the running time is multiplied by roughly $3^2$, etc.\\
    Along with the resulting graph of the test, we introduced a trend line (which is a $n^2$ function fitted to our test results) to show the running time is indeed $O(n^2)$.\\
    So, we can conclude that in practice this algorithm is $O(n^2)$ too.

    \subsection{Correct output}
    We analyzed a few test-cases that failed giving the correct output.\\
    We give suggestions as to what caused this behavior, and supply possible solutions below.\\

    \noindent An example of a test-case in which \textit{NearestNeighbor} fails is the Heart with 200 points. As you can see in Figure 2.3 there are two places the curve starts zigzagging between points.
    This problem occurs because the correct point in this case is not the nearest neighbor of the previous one. This could be corrected by taking the curve's current direction into account. This way, we could prevent the curve from jumping around.\\
    \noindent Another failed test-case is the Spiral with 91 points (Figure 2.4). Here, we experience the same problem as in the first case, that is, going in the wrong direction because the nearest point is not the correct successor. We also see another problem caused by this: when the spiral is nearly complete, the algorithm picks the points, which haven't been processed yet. In this case those points reside at the other side of the spiral.\\
    \noindent The third test-case is the ``8'' with 364 points (Figure 2.5). The problem here is that is does not intersect. Nearest Neighbor does not make sure there is at least one intersection. So, another thing we should take into consideration is that we need to check for intersections. \\
    \noindent The fourth and last test-case we discuss is the Flat with 178 points. This problem could also be solved by keeping track of the curve's current direction, so that it will not end up zigzagging again (marked in Figure 2.6).

        \includegraphics[scale = 0.5]{1NearestNeighbor/nnHeartgraph.png}
        \label{fig:nn_incorrectheart}
        \includegraphics[scale = 0.5]{1NearestNeighbor/nnSpiralgraph.png}\\
        \label{fig:nn_incorrectspiral}
        \qquad\qquad Figure 2.3: Incorrect heart \qquad\qquad\qquad Figure 2.4: Incorrect spiral
        {\ }\\[1.0cm]
        \includegraphics[scale = 0.5]{1NearestNeighbor/nnEightgraph.png}
        \label{fig:nn_incorrecteight}
        \includegraphics[scale = 0.5]{1NearestNeighbor/nnFlatgraph.png}\\
        \label{fig:nn_incorrectflat}
        \qquad\qquad Figure 2.5: Incorrect eight \qquad\qquad\qquad\qquad Figure 2.6: Incorrect flat
        
    \subsection{Conclusion}
    From all these test results we can conclude there are a number of things that need improving. We could, for example (as mentioned before), look at the curve's direction, which would fix the zigzagging. We should also check for intersections when the input requires it.